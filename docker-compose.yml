version: "3"

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - 127.0.0.1:9870:9870
      - 127.0.0.1:9010:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - hadoop_datasets:/home
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    env_file:
      - ./cluster_config_files/hadoop.env
    networks:
      etl_network:
        ipv4_address: 172.103.0.17

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    ports:
      - 127.0.0.1:9864:9864
    env_file:
      - ./cluster_config_files/hadoop.env
    networks:
      etl_network:
        ipv4_address: 172.103.0.2

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
    env_file:
      - ./cluster_config_files/hadoop.env
    networks:
      etl_network:
        ipv4_address: 172.103.0.3

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    env_file:
      - ./cluster_config_files/hadoop.env
    networks:
      etl_network:
        ipv4_address: 172.103.0.4

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./cluster_config_files/hadoop.env
    networks:
      etl_network:
        ipv4_address: 172.103.0.5

  spark-master:
    image: bde2020/spark-master:3.0.0-hadoop3.2
    container_name: spark-master
    depends_on:
      - namenode
      - datanode
    ports:
      - 127.0.0.1:8080:8080
      - 127.0.0.1:7077:7077
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      etl_network:
        ipv4_address: 172.103.0.6

  spark-worker-1:
    image: bde2020/spark-worker:3.0.0-hadoop3.2
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - 127.0.0.1:8081:8081
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      etl_network:
        ipv4_address: 172.103.0.7

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    depends_on:
      - namenode
      - datanode
    env_file:
      - ./cluster_config_files/hadoop-hive.env
    volumes:
      - hive_scripts:/home
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
      SERVICE_PRECONDITION: "hive-metastore:9083"
    ports:
      - 127.0.0.1:10000:10000
    networks:
      etl_network:
        ipv4_address: 172.103.0.8

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    env_file:
      - ./cluster_config_files/hadoop-hive.env
    command: /opt/hive/bin/hive --service metastore
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode:9864 hive-metastore-postgresql:5432"
    expose:
      - 9083
    networks:
      etl_network:
        ipv4_address: 172.103.0.9

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: hive-metastore-postgresql
    expose:
      - 5432
    networks:
      etl_network:
        ipv4_address: 172.103.0.10

  zoo:
    image: zookeeper:3.4.10
    container_name: zoo
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=0.0.0.0:2888:3888
    ports:
      - 127.0.0.1:2181:2181
    networks:
      etl_network:
        ipv4_address: 172.103.0.11
  
  mongodb:
    container_name: mongodb_etl
    restart: always
    image: 'mongo:4.4.6'
    volumes:
      - ./mongo_data:/data/db
    ports:
      - 127.0.0.1:27018:27017
    expose:
      - 27017
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongo mongo:27017/test --quiet
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 40s
    networks:
      etl_network:
        ipv4_address: 172.103.0.12

  neo4j:
    container_name: neo4j
    image: neo4j:latest
    ports:
       - 127.0.0.1:7474:7474
       - 127.0.0.1:7687:7687
    environment:
       - NEO4J_AUTH=neo4j/zeppelin
       - NEO4JLABS_PLUGINS=["apoc", "graph-data-science"]
    networks:
      etl_network:
        ipv4_address: 172.103.0.13
  
  zeppelin:
    container_name: zeppelin
    image: apache/zeppelin:0.9.0
    ports:
       - 127.0.0.1:8888:8080
    networks:
      etl_network:
        ipv4_address: 172.103.0.14
  
  postgres:
    image: postgres
    container_name: etl_postgres
    restart: always
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    ports:
      - 127.0.0.1:5444:5432
    volumes:
      - ./postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      etl_network:
        ipv4_address: 172.103.0.15
  
  minio:
    image: quay.io/minio/minio
    container_name: minio_object_storage
    volumes:
      - ./minio/data:/data
    ports:
      - 127.0.0.1:9000:9000
      - 127.0.0.1:9001:9001
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_ADDRESS: ':9000'
      MINIO_CONSOLE_ADDRESS: ':9001'
    command: minio server /data --console-address ":9090"
    networks:
      etl_network:
        ipv4_address: 172.103.0.16
      
volumes:
  hadoop_namenode:
  hadoop_datasets:
  hadoop_datanode:
  hadoop_historyserver:
  hive_scripts:
  mongo_data:
  postgres_data:

networks:
  etl_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.103.0.0/16
    name: etl_network